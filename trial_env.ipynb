{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Selection Methods project - Test Environment\n",
    "<b>Author: Yuval Uner</b><br/>\n",
    "<br/>\n",
    "In this notebook, you can use and experiment with the feature selection tools developed by me for the project.<br/>\n",
    "You can select a dataset, then try out and compare the different feature selection methods, and see how they affect the model's performance.<br/>"
   ],
   "id": "3a1a1566a6ef5c2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing libraries",
   "id": "865dc18d2080afd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import all of the libraries required for the project, both external libraries as well as the local libraries developed for the project.",
   "id": "4462377ad053dd4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## External libraries",
   "id": "46348e0ae5cadffe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error, max_error, log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, VarianceThreshold, SelectKBest, mutual_info_regression\n",
    "import seaborn as sns"
   ],
   "id": "2bd3b5ed2edf049",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Local libraries",
   "id": "a6c2f2b1567ce88c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from data_pre_processing import PreprocessingPipeline\n",
    "from filter_methods import WeightedCombination, VarianceInflationFactor\n",
    "from deep_network_methods import LinearAgent, SequentialAgent\n",
    "from shap_values_methods import BaseMethod, BranchingVariant, BacktrackingVariant"
   ],
   "id": "3e3d4a05b6ce2131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Select the dataset, preprocess the data, and choose the model and metrics\n",
    "Select one of the datasets below, load the data and and preprocess it, then select the relevant model and metrics to use for the task. <br/>\n",
    "The datasets are divided into 2 categories: linear regression datasets and classification datasets. <br/>\n",
    "For each category, there are 2 datasets to choose from. <br/>\n",
    "Please choose only one dataset, and follow the model selection and metric selection steps of the category it belongs to.<br/>\n",
    "<br/>\n",
    "All datasets go through the same preprocessing pipeline, which performs the following (in order):\n",
    "1. Convert categorical features to one-hot encoding\n",
    "2. Impute missing values by replacing them with the mean of the column\n",
    "3. Remove features with high collinearity between them. By default, this threshold is set to 0.9.\n",
    "At the end of the pipeline, the preprocessed data is returned, along with a list of which of the remaining columns are categorical and which are numerical.<br/>\n",
    "\n",
    "The data can then optionally be normalized using sklearn's StandardScaler.<br/>"
   ],
   "id": "929e4b935ef8adf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Linear Regression datasets\n",
    "You may choose of either of the 2 datasets below:\n",
    "1. Housing dataset. This is the dataset used in class. The same 10 features as used in the introduction lecture are used, as otherwise the dataset has too many features to handle in a reasonable amount of time by the more time consuming methods.\n",
    "2. WHO life expectancy (https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who). A dataset containing 21 features (20 in this project, as 1 is always dropped) and 2938 rows. It features statistical information about countries and health factors in those countries over several years, with the target variable being the life expectancy in the country."
   ],
   "id": "679716e38e9b901f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecting the dataset - please choose one of the following datasets",
   "id": "abfc17b2ebca8e28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Housing dataset",
   "id": "85721bc3a97b85a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the dataset and select only the relevant columns",
   "id": "84e4c11852a988ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/data_houses.csv', index_col=\"Id\")\n",
    "cols = [\"OverallQual\",\"GrLivArea\",\"GarageCars\",\"GarageArea\",\"TotalBsmtSF\",\"FullBath\",\"YearBuilt\",\"YearRemodAdd\",\n",
    "        \"LotFrontage\",\"MSSubClass\", \"SalePrice\"]\n",
    "data = data[cols]\n",
    "data"
   ],
   "id": "762fbadde2856798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the target variable",
   "id": "5b8c8d226fe0193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target = 'SalePrice'",
   "id": "e5a8ba13d4984b73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using the sequential agent - select the feature by which to sequence the data. This can be skipped if not using the sequential agent.<br/>\n",
    "In this case, the feature chosen is YearBuilt, as it may make sense that houses built in different years may have different distributions of features and different sale price distributions."
   ],
   "id": "d2db5fde4d52cba1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequence_by = 'YearBuilt'",
   "id": "c689a82e3b4afaf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, preprocess the data using the preprocessing pipeline.",
   "id": "ae470ca304bfdcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "preprocessor = PreprocessingPipeline(data, ['MSSubClass', 'FullBath'], 0.9, target_name='SalePrice')",
   "id": "24a4551cba6db633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data, continuous_cols, cat_cols = preprocessor.preprocess()",
   "id": "f156e28562e9f492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### WHO Life Expectancy",
   "id": "5832ecf3fc346a86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the dataset",
   "id": "f6bc9b9fcb52b77d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/Life Expectancy Data.csv')\n",
    "# Drop the country column, because it is categorical and with too many unique values.\n",
    "data = data.drop(columns=['Country'])\n",
    "data"
   ],
   "id": "5c73b3b9204fbc42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the target variable",
   "id": "471d3cf9d4890287"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target = 'Life expectancy '",
   "id": "da041937b474a07e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using the sequential agent - select the feature by which to sequence the data. This can be skipped if not using the sequential agent.<br/>\n",
    "In this case, the feature chosen is Year, as it may make sense for distributions to change over the years, and for worldwide trends to be reflected in the progress of years."
   ],
   "id": "35cb52f050c027aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequence_by = 'Year'",
   "id": "8ae490d33546743c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, preprocess the data using the preprocessing pipeline.",
   "id": "fd6211e9e9a9fb15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = PreprocessingPipeline(data,\n",
    "                                     [\n",
    "                                         'Status'\n",
    "                                     ],\n",
    "                                     0.9, target_name=target)"
   ],
   "id": "472dc31b7035c94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data, continuous_cols, cat_cols = preprocessor.preprocess()",
   "id": "e4a6d7373e474db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model and metric selection\n",
    "For the linear regression datasets, we use a linear regression model, MSE as the loss function, as well as R2, MAE, Mean Average Percent Error and max error as the evaluation metrics."
   ],
   "id": "21599b744b77f689"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LinearRegression()\n",
    "loss_func = mean_squared_error\n",
    "metrics = [r2_score, mean_absolute_error, mean_absolute_percentage_error, max_error]"
   ],
   "id": "1c6c007a68a524a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification datasets (Logistic Regression)",
   "id": "57f1473f11769eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You may choose of either of the 2 datasets below:\n",
    "1. German credit dataset (https://www.kaggle.com/datasets/mpwolke/cusersmarildownloadsgermancsv). A dataset containing 20 features and 1000 rows. It contains information about credit applicants, with the target variable being whether the applicant is creditworthy or not.\n",
    "2. Titanic dataset, imported via the seaborn library. A classic dataset, containing 14 features and 891 rows. It contains information about passengers on the Titanic, with the target variable being whether the passenger survived or not."
   ],
   "id": "deec1d37b60cf601"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecting the dataset - please choose one of the following datasets",
   "id": "be782c5b3bb654e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Telco customer churn dataset",
   "id": "1ca6a14f2b7a8459"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the dataset",
   "id": "62689b93f7871132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/german.csv', sep=';')\n",
    "data"
   ],
   "id": "9101792a09bfc88a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the target variable",
   "id": "6f3620c7e401acc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target = 'Creditability'",
   "id": "2b4c21733911f543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using the sequential agent - select the feature by which to sequence the data. This can be skipped if not using the sequential agent.<br/>\n",
    "In this case, the feature chosen is tenure, Age_years, as it may make sense for the features and creditability to change over the years."
   ],
   "id": "1255e46913ab794d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequence_by = 'Age_years'",
   "id": "8e73d65e2491cabe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, preprocess the data using the preprocessing pipeline.",
   "id": "6d38f83319a53db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = PreprocessingPipeline(data,\n",
    "                                     cat_cols=[\n",
    "                                         'Account_Balance', 'Payment_Status_of_Previous_Credit', 'Purpose', 'Value_Savings_Stocks',\n",
    "                                         'Sex_Marital_Status', 'Most_valuable_available_asset',\n",
    "                                         'Type_of_apartment', 'Occupation', 'Telephone', 'Foreign_Worker'\n",
    "                                     ],\n",
    "                                     threshold=0.9, target_name=target)\n",
    "data, continuous_cols, cat_cols = preprocessor.preprocess()"
   ],
   "id": "f23c162c96f94b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Titanic dataset",
   "id": "e94686e398e8b93e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the dataset",
   "id": "b7bea7b924a6e4b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = sns.load_dataset(\"titanic\")\n",
    "data"
   ],
   "id": "90d01432fd588fde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the target variable",
   "id": "3c5c77b8a3446c59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target = 'survived'",
   "id": "158f57fc46731446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using the sequential agent - select the feature by which to sequence the data. This can be skipped if not using the sequential agent. <br/>\n",
    "In this case, the selected feature is age, as it makes sense for the survival rate of passengers of different ages to be different."
   ],
   "id": "8dadf36df9ca1f85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequence_by = 'age'",
   "id": "adb02fa24e5f1d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, preprocess the data using the preprocessing pipeline.",
   "id": "e0a91969a8eb70d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = PreprocessingPipeline(data, [\n",
    "    'pclass', 'sex', 'sibsp', 'parch', 'embarked', \n",
    "    'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'\n",
    "], 0.9, target_name=target)\n",
    "data, continuous_cols, cat_cols = preprocessor.preprocess()"
   ],
   "id": "3d822c578c4fe0dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model and metric selection\n",
    "For the classification datasets, we use a logistic regression model, log loss as the loss function, as well as accuracy, precision, recall and F1 score as the evaluation metrics."
   ],
   "id": "e3cc4d7c88cf1c26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LogisticRegression()\n",
    "loss_func = log_loss\n",
    "metrics = [accuracy_score, precision_score, recall_score, f1_score]"
   ],
   "id": "31a83bb72e346fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalization - optional\n",
    "It is highly recommended to use this normalization step for the classification datasets, as sklearn's LogisticRegression model is sensitive to the scale of the features.<br/>\n",
    "On the other hand, for the linear regression datasets, this step is entirely optional, as the model used (LinearRegression) is not sensitive to the scale of the features."
   ],
   "id": "5dfa078e478cf65e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "data[continuous_cols] = scaler.fit_transform(data[continuous_cols])"
   ],
   "id": "1d5bdd12c976247a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split the data",
   "id": "4a83e44b5c1567e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the data into training, validation and test sets",
   "id": "39c198ac21bfe41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data[target], test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ],
   "id": "a6c438f6810722b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Selection",
   "id": "a2e54227a99bc9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filter methods",
   "id": "24ea9967d92c06bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Filter methods are methods that select features based on some statistical measure calculated from the data. They do not involve training a model, and are therefore computationally less expensive than wrapper methods. <br/>\n",
    "The filter methods implemented in this project are:\n",
    "1. A weighted sum of correlation and mutual information\n",
    "2. Variance Inflation Factor (VIF)\n",
    "\n",
    "Both methods have tunable hyperparameters, which may be adjusted or optimized automatically using the provided methods.<br/>\n",
    "<br/>\n",
    "The end of this section contains an evaluation, allowing you to compare the model's performance with and without feature selection using the selected method, as well as compare to sklearn's SelectKBest method."
   ],
   "id": "d4dfae47506eb34b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Weighted Sum Method",
   "id": "daa46221545ed6a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method description",
   "id": "9b4df35f6f6f825e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method computes a weighted sum between the mutual information and the correlation of each feature with the target variable. <br/>\n",
    "It has a weight parameter $\\alpha$, such that the score of each feature is computed as \n",
    "$$\\alpha \\cdot \\text{correlation} + (1-\\alpha) \\cdot \\text{mutual information}$$\n",
    "The value of $\\alpha$ is set to 0.5 by default, but can be adjusted by the user.<br/>\n",
    "<br/>\n",
    "The algorithm has 2 more hyperparameters - the maximum number of features to select, and a score threshold (features with a score below this threshold are removed).<br/>\n",
    "Both are set to 0 by default, but at least one of them must be set by the user."
   ],
   "id": "121b4f0a4a1da51b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method usage",
   "id": "646faf715fb952e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Basic usage",
   "id": "63fb949fd02ac7d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If the target was removed from the data, add it back to the data, as this method requires the target to be present in the data.",
   "id": "944de6f0a2cce489"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if target not in X_train.columns:\n",
    "    X_train[target] = y_train\n",
    "    X_val[target] = y_val\n",
    "    X_test[target] = y_test"
   ],
   "id": "d0ab9d8d9d62fb02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instantiate the WeightedCombination class, and fit it to the data.",
   "id": "a2a074e307214a9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_selector_weighted = WeightedCombination(correlation_weight=0.5)",
   "id": "a2c89b16be052834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_selector_weighted.fit(X_train, target_column=target, continuous_cols=continuous_cols,categorical_cols= cat_cols)",
   "id": "e473396f671d9b3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, you can either run the following cell select features using the current hyperparameters:",
   "id": "bf8930372e5159f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_after_selection, selected_features_weighted, feature_scores = feature_selector_weighted.transform(X_train, num_features=5, min_threshold=0.1)",
   "id": "dc0eda880fa3e966",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Or you can use the following cell to run the hyperparameter optimization process, which will run a grid search over the hyperparameters and select the best ones.<br/>\n",
    "By default, this process will not optimize the score threshold, as it is computationally expensive. If you wish to optimize it, set the optimize_threshold parameter to True."
   ],
   "id": "ce6df6b7404c7b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Hyperparameter optimization",
   "id": "6a32a5a818859686"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_transformed, selected_features_weighted, feature_scores, best_weight, best_loss, best_num_features, best_threshold, history = feature_selector_weighted.auto_optimize(X_train, y_train, X_val, y_val, model=model, loss_function=loss_func)",
   "id": "6a1f8ccbe30d11fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you ran the optimization process, you also get a history of the optimization process, which you can plot using the following cell.<br/>\n",
    "This gives a nice visualization of the loss as a function of the hyperparameters, showing if and how the model performs with less features, or with a different weight between correlation and mutual information."
   ],
   "id": "227917a558ad523c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weights, num_features = np.meshgrid(history.columns, history.index)\n",
    "plt.scatter(weights, num_features, c=history.values, cmap='viridis')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Number of features')\n",
    "plt.title('Loss vs. Weight and Number of Features')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "id": "b97565af5925b408",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Features and training a model",
   "id": "29da36d41659e6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Examine the selected features",
   "id": "d7d7d4d5438e28ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Number of features selected: {len(selected_features_weighted)}')\n",
    "print(f'Selected features: {selected_features_weighted}')"
   ],
   "id": "b19fed33bda9d358",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a model using the selected features.",
   "id": "486b530ecb43cde6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reset the model, to make sure it is not influenced by the previous training\n",
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features_weighted], y_train)\n",
    "y_pred = model.predict(X_test[selected_features_weighted])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_weighted = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_weighted[loss_func.__name__] = loss\n",
    "with_feature_selection_weighted['Number of features'] = len(selected_features_weighted)\n",
    "print(f\"Metrics with feature selection: {with_feature_selection_weighted}\")"
   ],
   "id": "aa75d836e9ff77e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Variance Inflation Factor",
   "id": "499824f43c61a128"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method description",
   "id": "a2abe3c92ee66972"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Variance Inflation Factor (VIF) is a measure of multicollinearity among the features. It measures how much the variance of the estimated coefficients of a feature is increased due to multicollinearity. <br/>\n",
    "Generally, a VIF of 1 indicates no multicollinearity, while a VIF of above 5-10 indicates high multicollinearity, and the feature should be removed. <br/>\n",
    "This method computes the VIF of each feature, and removes all features with a VIF above or below a certain threshold (up to the user's choice). The threshold is set to 5 by default, but can be adjusted by the user or optimized automatically.<br/>\n",
    "The method also has a hyperparameter to select the maximum number of features to keep, which is set to 0 by default, meaning all features with a VIF below the threshold will be kept.<br/>\n",
    "<br/>\n",
    "The user can choose whether the comparison direction is 'above' or 'below' the threshold, meaning whether to remove features with a VIF above or below the threshold. The default is 'above'.<br/>\n",
    "This is done because depending on the dataset, a high VIF may indicate that the feature contains important information, despite the multicollinearity, and from empirical observations, it seems that some datasets perform much better when removing features with a VIF below the threshold instead of above it."
   ],
   "id": "ffe108a555bcbf1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method usage",
   "id": "de2d8609133ae39b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Basic usage",
   "id": "7b3bb0c11b1fc09b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, remove the target feature from the data, if it was not already removed.",
   "id": "c605deff67926686"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if target in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "    X_val = X_val.drop(columns=[target])\n",
    "    X_test = X_test.drop(columns=[target])"
   ],
   "id": "ed8fe47281839c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instantiate the VarianceInflationFactor class, and fit it to the data.",
   "id": "154b19a5274afbbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_selector_vif = VarianceInflationFactor()",
   "id": "f4ee6a151114a519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_selector_vif.fit(X_train)",
   "id": "ebbea539ea25a104",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, you can either run the following cell select features using the current hyperparameters:",
   "id": "7261a6f047df32a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "selected_features_vif = feature_selector_vif.predict(X_train, threshold=5, num_features=0, less_than_threshold_comparison=True)",
   "id": "48cf083d138a3794",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Hyperparameter optimization",
   "id": "7d780ae35685057b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or you can use the following cell to run the hyperparameter optimization process, which will run a grid search over the hyperparameters and select the best ones.<br/>",
   "id": "7468d91d3457bee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "selected_features_vif, best_loss, best_threshold, best_num_features, best_comparison_direction,history_less_than_comparisons, history_greater_than_comparisons = feature_selector_vif.auto_optimize(X_train, y_train, X_val, y_val, model=model, loss_function=loss_func)",
   "id": "bbd64b322680dc3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you ran the optimization process, you also get a history of the optimization process, which you can plot using the following cell.<br/>\n",
    "This gives a nice visualization of the loss as a function of the hyperparameters, showing if and how the model performs with less features, with a different threshold, or with a different comparison direction."
   ],
   "id": "2e0a900a25b285cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "thresholds, num_features = np.meshgrid(history_less_than_comparisons.columns, history_less_than_comparisons.index)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].set_xlabel('Threshold')\n",
    "ax[0].set_ylabel('Number of features')\n",
    "ax[0].set_title('Loss vs. Threshold and Number of Features (VIF < threshold)')\n",
    "fig.colorbar(ax[0].scatter(thresholds, num_features, c=history_less_than_comparisons.values, cmap='viridis'))\n",
    "\n",
    "thresholds, num_features = np.meshgrid(history_greater_than_comparisons.columns, history_greater_than_comparisons.index)\n",
    "ax[1].set_xlabel('Threshold')\n",
    "ax[1].set_ylabel('Number of features')\n",
    "ax[1].set_title('Loss vs. Threshold and Number of Features (VIF > threshold)')\n",
    "fig.colorbar(ax[1].scatter(thresholds, num_features, c=history_greater_than_comparisons.values, cmap='viridis'))\n",
    "plt.show()"
   ],
   "id": "8a3c648c1866498c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Features and training a model",
   "id": "b86d0b603286df5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Examine the selected features",
   "id": "12b935811d599587"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Number of features selected: {len(selected_features_vif)}')\n",
    "print(f'Selected features: {selected_features_vif}')"
   ],
   "id": "9c14c7d99bfbf9e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a model using the selected features.",
   "id": "f6b6072add6f6de0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reset the model, to make sure it is not influenced by the previous training\n",
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features_vif], y_train)\n",
    "y_pred = model.predict(X_test[selected_features_vif])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_vif = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_vif[loss_func.__name__] = loss\n",
    "with_feature_selection_vif['Number of features'] = len(selected_features_vif)\n",
    "print(f\"Metrics with feature selection: {with_feature_selection_vif}\")"
   ],
   "id": "5027aab2cff8c8ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Method evaluation",
   "id": "7051a677694f8715"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Other methods used for comparison",
   "id": "ae4645e3bc8504b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, to evaluate the method, we first need to drop the target feature from the data.",
   "id": "8e4b4169b0485b2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if target in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "    X_val = X_val.drop(columns=[target])\n",
    "    X_test = X_test.drop(columns=[target])"
   ],
   "id": "b1d6b2b3097e5d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare to a baseline model, which uses all of the features.",
   "id": "bb1b1287bd99f0b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "loss = loss_func(y_test, y_pred)\n",
    "without_feature_selection = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "without_feature_selection[loss_func.__name__] = loss\n",
    "without_feature_selection['Number of features'] = len(X_train.columns)\n",
    "print(f\"Metrics without feature selection: {without_feature_selection}\")"
   ],
   "id": "c36f0fb412b211a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare to sklearn's VarianceThreshold method (sklearn's model agnostic filter method), with the threshold set to 0.8, same as in the example on the documentation.",
   "id": "3da49b55d200ad1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selector = VarianceThreshold(threshold=0.8)\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features_sklearn = X_train.columns[selector.get_support()]\n",
    "print(f'Selected features using VarianceThreshold: {selected_features_sklearn}')"
   ],
   "id": "fdbcbe040b043103",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features_sklearn], y_train)\n",
    "y_pred = model.predict(X_test[selected_features_sklearn])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_sklearn_var_thresh = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_sklearn_var_thresh[loss_func.__name__] = loss\n",
    "with_feature_selection_sklearn_var_thresh['Number of features'] = len(selected_features_sklearn)\n",
    "print(f\"Metrics with feature selection using VarianceThreshold: {with_feature_selection_sklearn_var_thresh}\")"
   ],
   "id": "d348ff359ce769a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, compare to sk_learn's SelectKBest as well, using mutual_info_regression as the score function. This method is not model agnostic, but it is still a filter method and can be used for comparison.",
   "id": "281a31f876f6f121"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selector = SelectKBest(score_func=mutual_info_regression)\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features_sklearn = X_train.columns[selector.get_support()]\n",
    "print(f'Selected features using SelectKBest: {selected_features_sklearn}')"
   ],
   "id": "6b5b6c60a429f342",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features_sklearn], y_train)\n",
    "y_pred = model.predict(X_test[selected_features_sklearn])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_sklearn_k_best = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_sklearn_k_best[loss_func.__name__] = loss\n",
    "with_feature_selection_sklearn_k_best['Number of features'] = len(selected_features_sklearn)\n",
    "print(f\"Metrics with feature selection using SelectKBest: {with_feature_selection_sklearn_k_best}\")"
   ],
   "id": "feac2da7a6f61556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Results",
   "id": "1b245c3dea2938ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you only ran the cells for the weighted sum method, run this cell to format the results into a table",
   "id": "9c6223e6421dff95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame([with_feature_selection_weighted, without_feature_selection, with_feature_selection_sklearn_var_thresh, with_feature_selection_sklearn_k_best], index=[f'Feature selection: {feature_selector_weighted.__name__}', 'Without feature selection', 'sklearn Variance Threshold', 'sklearn SelectKBest'])\n",
    "results"
   ],
   "id": "7e1a7216e38da512",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you only ran the cells for the VIF method, run this cell to format the results into a table",
   "id": "c3bc5e8b02d984f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame([with_feature_selection_vif, without_feature_selection, with_feature_selection_sklearn_var_thresh, with_feature_selection_sklearn_k_best], index=[f'Feature selection: {feature_selector_vif.__class__.__name__}', 'Without feature selection', 'sklearn Variance Threshold', 'sklearn SelectKBest'])\n",
    "results"
   ],
   "id": "615b5ab6d82f9f7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you ran both methods, run this cell to format the results into a table",
   "id": "5d5c81f0b03b2040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame([with_feature_selection_weighted, with_feature_selection_vif, without_feature_selection, with_feature_selection_sklearn_var_thresh, with_feature_selection_sklearn_k_best], index=[f'Feature selection: {feature_selector_weighted.__class__.__name__}', f'Feature selection: {feature_selector_vif.__class__.__name__}', 'Without feature selection', 'sklearn Variance Threshold', 'sklearn SelectKBest'])\n",
    "results"
   ],
   "id": "e519bfc3ce90e262",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Wrapper methods using deep reinforcement learning\n",
    "Wrapper methods are methods that select features by training a model, and using the model's performance as a measure of the feature's importance, treating the model as a black box. <br/>\n",
    "<br/>\n",
    "Feature selection can be described as a prediction problem, where a model needs to predict which subset of features will lead to the best performance. <br/>\n",
    "To do this in an interpretable way using neural networks, there needs to be a \"disconnect\" between the feature selection model and the model that uses the selected features, thus making it impossible to directly compute gradients for the feature selection model. <br/>\n",
    "Thus, reinforcement learning was used.<br/>\n",
    "<br/>\n",
    "There are 2 methods implemented in this project:\n",
    "1. LinearAgent\n",
    "2. SequentialAgent\n",
    "\n",
    "In the case of these algorithms, the prediction model is used to compute the reward for the feature selection model, and the feature selection model is trained using reinforcement learning. <br/>\n",
    "<br/>\n",
    "All methods were implemented using the StableBaselines3 library, which is a PyTorch-based library for reinforcement learning."
   ],
   "id": "2781a229e9db77bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LinearAgent",
   "id": "a5d12ba434f6c1b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method description",
   "id": "26939957a44c2dbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LinearAgent uses a linear model as a feature extractor for the feature selection task. <br/>\n",
    "The agent is trained via either the A2C or PPO policies.<br/>\n",
    "The environment's action space is a binary vector, where each element corresponds to a feature, and the agent can choose to select or not select each feature.<br/>\n",
    "The environment's observation space is a batch of data.<br/>\n",
    "Each step the agent predicts features by a batch of data, and the environment returns the reward, which is the performance of the downstream model on the selected features. An episode is a full pass over the data (one epoch).<br/>\n",
    "<br/>\n",
    "It uses a default network architecture, where the input layer is the number of features, and each subsequent layer has half the number of neurons as the previous layer, until reaching the size of the output layer (the number of features). Additionally, the network uses ReLU activations and has a layer normalization layer after every 2 layers.<br/>\n",
    "<br/>\n",
    "the user can customize Both the network and the environment, by passing the relevant classes to the agent's constructor.<br/>"
   ],
   "id": "72e80255a8a70b24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method usage",
   "id": "b272e992b0c0b9a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Training the agent",
   "id": "9c7098911ccdc98e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, remove the target feature from the data, if it was not already removed.",
   "id": "7a6ba47329b5d25a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if target in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "    X_val = X_val.drop(columns=[target])\n",
    "    X_test = X_test.drop(columns=[target])"
   ],
   "id": "ad6da88895a23584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set the batch size for the agent",
   "id": "913202fa586dc58f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batch_size = 256",
   "id": "2a132806ef8940b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instantiate the LinearAgent class",
   "id": "5cfd3fbf80d0312a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lin_agent = LinearAgent(X_train, y_train, LinearRegression(), mean_squared_error, batch_size=batch_size, agent_type='A2C',save_path=\"models/linear_agent\", eval_freq=500)\n",
    "print(lin_agent.agent.policy)"
   ],
   "id": "e8630f2f1ffc4287",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Train the agent over a certain number of steps.\n",
    "Note: this may take a long time."
   ],
   "id": "2c01bd811062e944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lin_agent.learn(num_steps=4000)",
   "id": "7444a6abdc054ed0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Optionally, save the model. This is useful if you want to continue training later, or if you want to load the model for evaluation.<br/>\n",
    "There is a load method, which can be used to load the model."
   ],
   "id": "5c3ced90b1e74895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lin_agent.save(model_name=\"end_of_training\")",
   "id": "59c42925c4842442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Predicting features and training a model",
   "id": "108506b53a91f599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predict the features on a batch of data, sampled from the validation data",
   "id": "2c7b569ca8380809"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_val_sample = X_val.sample(batch_size, replace=True)",
   "id": "fd49e7f07cbcfe10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting deterministic to True will make the agent always select the same features for the same data, while setting it to False will allow the agent to select a different (potentially similar) set of features for the same data, potentially improving the model's performance.\n",
   "id": "492ab28cfeac671d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "action = lin_agent.predict(X_val_sample, deterministic=True)\n",
    "selected_features = X_train.columns[action == 1]\n",
    "print(f'Number of selected features: {len(selected_features)}, Selected features: {selected_features}')"
   ],
   "id": "49936702f805510e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Run the cell below to train a model using the selected features.<br/>\n",
    "The evaluation when compared to other methods can be done further below, beneath the sequential agent section."
   ],
   "id": "13a3b8e3f7d8779e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reset the model, to make sure it is not influenced by the previous training\n",
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features], y_train)\n",
    "y_pred = model.predict(X_test[selected_features])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection[loss_func.__name__] = loss\n",
    "with_feature_selection['Number of features'] = len(selected_features)\n",
    "print(f\"Metrics with feature selection: {with_feature_selection}\")"
   ],
   "id": "6afae789d14567cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SequentialAgent",
   "id": "f81e37d9113a41bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method description",
   "id": "b4f99903596926ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "SequentialAgent treats the data as a collection of sequences, where each sequence is a subset of the data, sorted and separated by a certain feature. <br/>\n",
    "It can use either a linear neural network or a bi-directional self-attentive LSTM network as the feature extractor for the feature selection task with either the A2C or PPO policies, or use the RecurrentPPO policy with a linear feature extractor and a LSTM as the policy network.<br/>\n",
    "The default uses the A2C policy with a LSTM network as the feature extractor.<br/>\n",
    "This method can select different features for different sequences (although it may also select the same features for all sequences), which can then be used for training several models, each for a different sequence.<br/>\n",
    "For example, in the housing dataset, a sequence could be all houses built between 1850 and 1900, and another sequence could be all houses built between 1900 and 1950.<br/>"
   ],
   "id": "5285f4390b8dc6d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method usage",
   "id": "f4de9faca81987f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Training the agent",
   "id": "d87a2ad7ed757e65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, remove the target feature from the data, if it was not already removed.",
   "id": "6baada4d4dc01366"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if target in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "    X_val = X_val.drop(columns=[target])\n",
    "    X_test = X_test.drop(columns=[target])"
   ],
   "id": "577dac9752df2411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instantiate the SequentialAgent class",
   "id": "196084c6ed88550e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sequential_agent = SequentialAgent(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    col=sequence_by,\n",
    "    downstream_model=model,\n",
    "    loss_function=loss_func,\n",
    "    agent_type='A2C',\n",
    "    network_type='recurrent',\n",
    "    save_path='models/sequential_agent',\n",
    "    eval_freq=500,\n",
    "    lstm_hidden_layer_size=64,\n",
    "    lstm_num_layers=2,\n",
    "    clustering_method='MeanShift'\n",
    ")\n",
    "print(sequential_agent.agent.policy)"
   ],
   "id": "e07883c84cc9d01b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is possible to examine the sequences the model will use for training by using the sequencer object, which can be obtained by calling the get_sequencer method on the agent.",
   "id": "ce2cb00fb28b44b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sequencer = sequential_agent.get_sequencer()\n",
    "train_sequences, train_targets = sequencer.sequence_by_ranges(X_train, y_train)\n",
    "print(f'Number of sequences: {len(train_sequences)}')\n",
    "print(f'Sequence lengths: {[len(sequence) for sequence in train_sequences]}')"
   ],
   "id": "5fcecd274d63e4ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Train the agent for a specified number of steps.\n",
    "Note: this may take a long time."
   ],
   "id": "f212254c87dadf6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequential_agent.learn(num_steps=2000)",
   "id": "f6452112804bf7b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Optionally - save the model",
   "id": "460b2a09d3424a8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequential_agent.save(model_name=\"end_of_training\")",
   "id": "197d02fb167574fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Predicting features and training models",
   "id": "dc2c833e3deb69aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the agent's predictions on the validation set, then train models on the predicted feature set.",
   "id": "8c08c564cceb30b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = sequential_agent.predict(X_train, deterministic=True)\n",
    "for prediction in predictions:\n",
    "    print(X_train.columns[prediction == 1])"
   ],
   "id": "c8b325b4755c0a0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize a list of models, then sequence the training and test data by the ranges of sequences found on the training set.",
   "id": "6d51f97073fce7e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_with_sequencing_and_feature_selection = []\n",
    "metrics_with_sequencing_and_feature_selection = []\n",
    "sequencer = sequential_agent.get_sequencer()\n",
    "train_sequences, train_targets = sequencer.sequence_by_ranges(X_train, y_train)\n",
    "test_sequences, test_targets = sequencer.sequence_by_ranges(X_test, y_test)"
   ],
   "id": "155c0bffaa270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(train_sequences)):\n",
    "    X_train_sequence = train_sequences[i]\n",
    "    # Choose the features selected by the agent\n",
    "    X_train_sequence = X_train_sequence[X_train_sequence.columns[predictions[i] == 1]]\n",
    "    y_train_sequence = train_targets[i]\n",
    "    X_test_sequence = test_sequences[i]\n",
    "    X_test_sequence = X_test_sequence[X_test_sequence.columns[predictions[i] == 1]]\n",
    "    y_test_sequence = test_targets[i]\n",
    "    \n",
    "    # If any sequence is empty, we skip training a model on it. An empty sequence indicates that any samples in the sequence range are outliers.\n",
    "    # This is an edge case, which should normally not happen, but can happen if the dataset has either a very small number of samples or a small number of outliers that could not be grouped together.\n",
    "    if X_train_sequence.shape[0] == 0 or X_test_sequence.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    model = model.__class__()\n",
    "    model.fit(X_train_sequence, y_train_sequence)\n",
    "    y_pred = model.predict(X_test_sequence)\n",
    "    \n",
    "    loss = loss_func(y_test_sequence, y_pred)\n",
    "    sequence_metrics = {metric.__name__: metric(y_test_sequence, y_pred) for metric in metrics}\n",
    "    sequence_metrics[loss_func.__name__] = loss\n",
    "    sequence_metrics['Number of features'] = len(X_train_sequence.columns)\n",
    "    \n",
    "    metrics_with_sequencing_and_feature_selection.append(sequence_metrics)\n",
    "    models_with_sequencing_and_feature_selection.append(model)"
   ],
   "id": "89f19bdf92fcb9fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for metric_collection in metrics_with_sequencing_and_feature_selection:\n",
    "    print(metric_collection)"
   ],
   "id": "69c0a2ad960e90be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute and print average, max and min metrics along all of the models",
   "id": "ac766077ea558156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_metrics_with_sequencing_with_feature_selection = {metric: np.mean([model_metrics[metric] for model_metrics in metrics_with_sequencing_and_feature_selection]) for metric in\n",
    "                   metrics_with_sequencing_and_feature_selection[0].keys()}\n",
    "max_metrics_with_sequencing_with_feature_selection = {metric: np.max([model_metrics[metric] for model_metrics in metrics_with_sequencing_and_feature_selection]) for metric in\n",
    "                   metrics_with_sequencing_and_feature_selection[0].keys()}\n",
    "min_metrics_with_sequencing_with_feature_selection = {metric: np.min([model_metrics[metric] for model_metrics in metrics_with_sequencing_and_feature_selection]) for metric in\n",
    "                   metrics_with_sequencing_and_feature_selection[0].keys()}\n",
    "print(f\"Average metrics: {avg_metrics_with_sequencing_with_feature_selection}\")\n",
    "print(f\"Max metrics: {max_metrics_with_sequencing_with_feature_selection}\")\n",
    "print(f\"Min metrics: {min_metrics_with_sequencing_with_feature_selection}\")"
   ],
   "id": "ad8a7d9c788b1d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For comparison, train a model with the same sequences, but without feature selection.",
   "id": "d892a3757fa02aef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_with_sequencing_and_without_feature_selection = []\n",
    "metrics_with_sequencing_and_without_feature_selection = []"
   ],
   "id": "4b02ce5b3657abe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(train_sequences)):\n",
    "    X_train_sequence = train_sequences[i]\n",
    "    y_train_sequence = train_targets[i]\n",
    "    X_test_sequence = test_sequences[i]\n",
    "    y_test_sequence = test_targets[i]\n",
    "    \n",
    "    # If any sequence is empty, we skip training a model on it. An empty sequence indicates that any samples in the sequence range are outliers.\n",
    "    # This is an edge case, which should normally not happen, but can happen if the dataset has either a very small number of samples or a small number of outliers that could not be grouped together.\n",
    "    if X_train_sequence.shape[0] == 0 or X_test_sequence.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    model = model.__class__()\n",
    "    model.fit(X_train_sequence, y_train_sequence)\n",
    "    y_pred = model.predict(X_test_sequence)\n",
    "    \n",
    "    loss = loss_func(y_test_sequence, y_pred)\n",
    "    sequence_metrics = {metric.__name__: metric(y_test_sequence, y_pred) for metric in metrics}\n",
    "    sequence_metrics[loss_func.__name__] = loss\n",
    "    sequence_metrics['Number of features'] = len(X_train_sequence.columns)\n",
    "    \n",
    "    metrics_with_sequencing_and_without_feature_selection.append(sequence_metrics)\n",
    "    models_with_sequencing_and_without_feature_selection.append(model)"
   ],
   "id": "aa0316fb4f1b79f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for metric_collection in metrics_with_sequencing_and_without_feature_selection:\n",
    "    print(metric_collection)"
   ],
   "id": "aa38512ce165540b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_metrics_with_sequencing_without_feature_selection = {metric: np.mean([model_metrics[metric] for model_metrics in metrics_with_sequencing_and_without_feature_selection]) for metric in\n",
    "                   metrics_with_sequencing_and_without_feature_selection[0].keys()}\n",
    "max_metrics_with_sequencing_without_feature_selection = {metric: np.max([model_metrics[metric] for model_metrics in metrics_with_sequencing_and_without_feature_selection]) for metric in\n",
    "                     metrics_with_sequencing_and_without_feature_selection[0].keys()}\n",
    "min_metrics_with_sequencing_without_feature_selection = {metric: np.min([model_metrics[metric] for model_metrics in metrics_with_sequencing_and_without_feature_selection]) for metric in\n",
    "                        metrics_with_sequencing_and_without_feature_selection[0].keys()}\n",
    "print(f\"Average metrics: {avg_metrics_with_sequencing_without_feature_selection}\")\n",
    "print(f\"Max metrics: {max_metrics_with_sequencing_without_feature_selection}\")\n",
    "print(f\"Min metrics: {min_metrics_with_sequencing_without_feature_selection}\")"
   ],
   "id": "e2b5060bf224e1aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Comparative method evaluation",
   "id": "e53a349e1331b8f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Other methods used for comparison",
   "id": "2ead0c76c0846461"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare to a baseline model, which uses all of the features",
   "id": "6fca443f33edb499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "loss = loss_func(y_test, y_pred)\n",
    "without_feature_selection = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "without_feature_selection[loss_func.__name__] = loss\n",
    "without_feature_selection['Number of features'] = len(X_train.columns)\n",
    "print(f\"Metrics without feature selection: {without_feature_selection}\")"
   ],
   "id": "8ce714eb94fd3db9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare to sklearn's Sequential Feature Selector, a wrapper method that chooses features in an iterative process, with the default settings.",
   "id": "6c8227e58c7fc9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selector = SequentialFeatureSelector(model.__class__())\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features_sklearn = X_train.columns[selector.support_]\n",
    "print(f'Selected features using Sequential Feature Selector: {selected_features_sklearn}')"
   ],
   "id": "f8f6d9c882ab909f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features_sklearn], y_train)\n",
    "y_pred = model.predict(X_test[selected_features_sklearn])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_sklearn = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_sklearn[loss_func.__name__] = loss\n",
    "with_feature_selection_sklearn['Number of features'] = len(selected_features_sklearn)\n",
    "print(f\"Metrics with feature selection using Sequential Feature Selector: {with_feature_selection_sklearn}\")"
   ],
   "id": "c5def36e0bd05d8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Results",
   "id": "388d66ee3bc225dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you used the linear agent, run this cell to format the results into a table",
   "id": "475248a8339b0b4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame([with_feature_selection, without_feature_selection, with_feature_selection_sklearn], index=[f'Feature selection: {lin_agent.__class__.__name__}', 'Without feature selection', 'Sequential Feature Selector'])\n",
    "results"
   ],
   "id": "5d0ac710d51eb280",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you used the sequential agent, run this cell to format the results into a table",
   "id": "d317cd4db063c693"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame([avg_metrics_with_sequencing_with_feature_selection, max_metrics_with_sequencing_with_feature_selection, min_metrics_with_sequencing_with_feature_selection, avg_metrics_with_sequencing_without_feature_selection, max_metrics_with_sequencing_without_feature_selection, min_metrics_with_sequencing_without_feature_selection, without_feature_selection, with_feature_selection_sklearn], index=['Average metrics with sequencing and feature selection', 'Max metrics with sequencing and feature selection', 'Min metrics with sequencing and feature selection', 'Average metrics with sequencing and without feature selection', 'Max metrics with sequencing and without feature selection', 'Min metrics with sequencing and without feature selection', 'Without feature selection', 'Sequential Feature Selector'])\n",
    "results"
   ],
   "id": "6d3de7c80654e633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Wrapper methods using SHAP values",
   "id": "4bde599a52bee0d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This section introduces two variants of the algorithm presented in the paper \"A feature selection method based on Shapley values robust to\n",
    "    concept shift in regression\" by Carlos Sebastin and Carlos E. Gonzlez-Guilln, arXiv preprint arXiv:2304.14774 (2023) (https://arxiv.org/abs/2304.14774).<br/>\n",
    "<br/>\n",
    "The algorithm presented in the paper is composed of 2 phases:\n",
    "1. The optional preliminary phase\n",
    "2. The main phase\n",
    "\n",
    "In the optional preliminary phase, the algorithm introduces a random variable to the data, then removes all features that have less influence on the model than the random variable.<br/>\n",
    "This can help to greatly reduce the number of features while losing minimal information. However, if too many features are removed, the model may lose important information, and the model's performance may decrease.<br/>\n",
    "<br/>\n",
    "In the main phase of the algorithm, the algorithm sequentially removes the feature that has the highest negative influence on the model among the currently selected features.<br/>\n",
    "This iterates until either a desired number of features are removed or until there are no more features to remove.<br/>\n",
    "At the end of the main phase, the algorithm returns the best subset of features found during the process.<br/>\n",
    "<br/>\n",
    "For the full and in-depth explanation of the algorithm, please refer to the paper.<br/>\n",
    "<br/>\n",
    "The algorithm was implemented in this project, with 2 variants that modify the main phase:\n",
    "1. Branching\n",
    "2. Backtracking"
   ],
   "id": "df9ce0ec9944234b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before procedding, drop the target feature from the data, if it was not already removed.",
   "id": "4f3abc9eec046341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if target in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "    X_val = X_val.drop(columns=[target])\n",
    "    X_test = X_test.drop(columns=[target])"
   ],
   "id": "9711c63595c30259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Also, run this cell to initialize the metric dictionaries for all of the methods.\n",
    "This will be used later to create a table, without having to run the evaluation cells for each method."
   ],
   "id": "82c8e5f068c14d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with_feature_selection_base = {metric.__name__: np.nan for metric in metrics}\n",
    "with_feature_selection_branching = {metric.__name__: np.nan for metric in metrics}\n",
    "with_feature_selection_backtracking = {metric.__name__: np.nan for metric in metrics}"
   ],
   "id": "efa0461c60bedabe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base method",
   "id": "24fe605dcfa1b59b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The base method is an implementation of the algorithm described in the paper.<br/>\n",
    "It can be used as follows:"
   ],
   "id": "188508f5670df7c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Instantiate the BaseMethod class and predict the features.<br/>\n",
    "The hyperparamters $q_{low}$ and $q_{high}$ are used for quantile selection. For the exact explanation of these hyperparameters, please refer to the paper."
   ],
   "id": "94591751cb5d5e2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_method = BaseMethod(q_low=0.15, q_high=0.85)",
   "id": "696ebd960a8026bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_method_features, best_score = base_method.predict(X_train, y_train, X_val, y_val, model, loss_func, num_iter_prev=1)",
   "id": "993f50ea82ee4dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observe the selected features",
   "id": "78fdd77bebeeec72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Number of features: {len(base_method_features)}\")\n",
    "print(f\"Features: {base_method_features}\")"
   ],
   "id": "b320984aed93919e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a model using the selected features.",
   "id": "43d638dd4b93221b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[base_method_features], y_train)\n",
    "y_pred = model.predict(X_test[base_method_features])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_base = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_base[loss_func.__name__] = loss\n",
    "with_feature_selection_base['Number of features'] = len(base_method_features)\n",
    "print(f\"Metrics with feature selection: {with_feature_selection_base}\")"
   ],
   "id": "cb30a777878b6886",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Branching variant",
   "id": "8bfb8ad0f5240c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method description",
   "id": "21db9cea67da07c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the branching variant, the algorithm creates 2 branches at each iteration of the main phase, one where the feature with the highest negative influence is removed, and one where the feature with the second highest negative influence is removed, thus allowing the algorithm to explore much more of the search space.<br/>\n",
    "This can lead to better results, but may also be more computationally expensive.<br/> "
   ],
   "id": "7c60f132fa07afd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method usage",
   "id": "1ed1e1e2cd995fd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Instantiate the BranchingVariant class and predict the best feature subset.<br/>\n",
    "Note: this may take some time to run, particularly so on datasets with a large number of features."
   ],
   "id": "1a28376825bbbc9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "branching_method = BranchingVariant(q_low=0.15, q_high=0.85)",
   "id": "1cad1c707a118516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "branching_method_features, best_score = branching_method.predict(X_train, y_train, X_val, y_val, model, loss_func, num_iter_prev=1)",
   "id": "90114b774c9fa37f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observe the selected features",
   "id": "ed70e0db7d6e8332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Number of features: {len(branching_method_features)}\")\n",
    "print(f\"Features: {branching_method_features}\")"
   ],
   "id": "4ab80184b6dfce0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a model using the selected features.",
   "id": "d9934e5be66ae001"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[branching_method_features], y_train)\n",
    "y_pred = model.predict(X_test[branching_method_features])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_branching = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_branching[loss_func.__name__] = loss\n",
    "with_feature_selection_branching['Number of features'] = len(branching_method_features)\n",
    "print(f\"Metrics with feature selection: {with_feature_selection_branching}\")\n"
   ],
   "id": "cbd868c3a28270cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Backtracking variant",
   "id": "b90144815a79827f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method description",
   "id": "f22102fb770abdd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The algorithm described in the paper is a Sequential Feature Selection (SFS) algorithm.<br/>\n",
    "This variant transforms it into a Sequential Floating Forward Selection (SFFS) algorithm, by adding a backtracking mechanism.<br/>\n",
    "The backtracking mechanism allows the algorithm to add previously removed features back to the selected features, if it improves the model's performance.<br/>"
   ],
   "id": "3c3c16375976bd53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Method usage",
   "id": "956d45cc670644bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Instantiate the BacktrackingVariant class and predict the best feature subset.<br/>\n",
    "Note: this may take some time to run, particularly so on datasets with a large number of features."
   ],
   "id": "9513b0b0a91e5fe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "backtracking_method = BacktrackingVariant(q_low=0.15, q_high=0.85)",
   "id": "d765c5a48d75b744",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "backtracking_method_features, best_score = backtracking_method.predict(X_train, y_train, X_val, y_val, model, loss_func, num_iter_prev=1)",
   "id": "7ec70ec7721eaa86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observe the selected features",
   "id": "362a8ed5b06977cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Number of features: {len(backtracking_method_features)}\")\n",
    "print(f\"Features: {backtracking_method_features}\")"
   ],
   "id": "9d2f6854f174e334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train a model using the selected features.",
   "id": "97469ef868e3da56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[backtracking_method_features], y_train)\n",
    "y_pred = model.predict(X_test[backtracking_method_features])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_backtracking = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_backtracking[loss_func.__name__] = loss\n",
    "with_feature_selection_backtracking['Number of features'] = len(backtracking_method_features)\n",
    "print(f\"Metrics with feature selection: {with_feature_selection_backtracking}\")"
   ],
   "id": "d6af16802d98427b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Method evaluation",
   "id": "a796ac811cc11cc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Other methods used for comparison",
   "id": "25d6bfb4dd00c094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare to a baseline model, which uses all of the features",
   "id": "f65c2427eaab43e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "loss = loss_func(y_test, y_pred)\n",
    "without_feature_selection = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "without_feature_selection[loss_func.__name__] = loss\n",
    "without_feature_selection['Number of features'] = len(X_train.columns)\n",
    "print(f\"Metrics without feature selection: {without_feature_selection}\")"
   ],
   "id": "d6e68993b08ff278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compare to sklearn's Sequential Feature Selection method, with the direction set to backward, to make it as close in behavior to the algorithm as possible.",
   "id": "7eda6c58f0265e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selector = SequentialFeatureSelector(model.__class__(), direction='backward')\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features_sklearn = X_train.columns[selector.support_]\n",
    "print(f'Selected features using Sequential Feature Selector: {selected_features_sklearn}')"
   ],
   "id": "453288d777cc950a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = model.__class__()\n",
    "model.fit(X_train[selected_features_sklearn], y_train)\n",
    "y_pred = model.predict(X_test[selected_features_sklearn])\n",
    "loss = loss_func(y_test, y_pred)\n",
    "with_feature_selection_sklearn = {metric.__name__: metric(y_test, y_pred) for metric in metrics}\n",
    "with_feature_selection_sklearn[loss_func.__name__] = loss\n",
    "with_feature_selection_sklearn['Number of features'] = len(selected_features_sklearn)\n",
    "print(f\"Metrics with feature selection using Sequential Feature Selector: {with_feature_selection_sklearn}\")"
   ],
   "id": "9544cd4e451c1cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Results",
   "id": "504416e392f0fce1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Format the results in a table.",
   "id": "db29531fa449fb78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame([with_feature_selection_base, with_feature_selection_branching, with_feature_selection_backtracking, without_feature_selection, with_feature_selection_sklearn], index=['Base method', 'Branching variant', 'Backtracking variant', 'Without feature selection', 'sklearn Sequential Feature Selector (backwards direction)'])\n",
    "results"
   ],
   "id": "c695b0e2bae28253",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
